{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base Classification\n",
    "\n",
    "## Load imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make common scripts visible and unsupervised classifier code\n",
    "import sys\n",
    "sys.path.append('../common/')\n",
    "sys.path.append('../kb-classifier/')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "\n",
    "from loader import load_preprocessed_data\n",
    "from lookup_tables import topic_code_to_topic_dict, topic_code_to_int, int_to_topic_code\n",
    "from kb_classifier import KnowledgeBasePredictor\n",
    "from kb_common import int_to_topic, wiki_topics_to_actual_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72488\n",
      "[20. 20. 20. 20. 20. 20.]\n"
     ]
    }
   ],
   "source": [
    "x, y = load_preprocessed_data('data/rcv1_no_stopwords.csv')\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# Get 20% test\n",
    "total_examples = len(y)\n",
    "split_point = int(total_examples * 0.8)\n",
    "test_x = x[split_point:]\n",
    "test_y = y[split_point:]\n",
    "\n",
    "# Take N documents of each type from the training set for classifier tuning\n",
    "train_x = []\n",
    "train_y = np.zeros(shape=120)\n",
    "\n",
    "counts = np.zeros(shape=len(topic_code_to_int.keys()))\n",
    "current_index = 0\n",
    "print(split_point)\n",
    "for i in range(split_point):\n",
    "    topic_int = y[i]\n",
    "    \n",
    "    if counts[topic_int] < 20:\n",
    "        train_x.append(x[i])\n",
    "        train_y[current_index] = topic_int\n",
    "        counts[topic_int] += 1\n",
    "        current_index += 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise and tune class probabilities for unsupervised learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki topic probabilies shape: (120, 300)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "kb_predictor = KnowledgeBasePredictor(topic_code_to_topic_dict.values(), topic_depth=1)\n",
    "#kb_predictor.fit_tfidf(train_x)\n",
    "kb_predictor.train(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = kb_predictor.predict(train_x)\n",
    "classification_report, confusion_matrix = kb_predictor.get_classification_report(train_y, predict_y)\n",
    "\n",
    "print(classification_report)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at mean topic probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 120",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6a164c012e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot graph of mean topic probabilities for each topic class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mint_to_topic_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprob_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkb_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_class_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_y\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprob_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkb_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_class_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_y\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprob_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 120"
     ]
    }
   ],
   "source": [
    "prob_means = np.zeros(shape=(6, 6))\n",
    "\n",
    "# Plot graph of mean topic probabilities for each topic class\n",
    "for index, topic_code in int_to_topic_code.items():\n",
    "    prob_mean = np.mean(kb_predictor.last_class_probabilities[train_y == index], axis=0)\n",
    "    prob_std = np.std(kb_predictor.last_class_probabilities[train_y == index], axis=0)\n",
    "    prob_means[index] = prob_mean\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Mean Topic Probabilies for {} Articles'.format(topic_code_to_topic_dict[topic_code]))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.xlim(0.0, 0.6)\n",
    "    sns.barplot(x=prob_mean, y=list(int_to_topic.values()))\n",
    "    plt.savefig('topic_prob_{}.pdf'.format(topic_code_to_topic_dict[topic_code]), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot graph of mean topic probabilities for each topic class\n",
    "for index, topic_code in int_to_topic_code.items():\n",
    "    prob_mean = np.mean(kb_predictor.last_class_probabilities[train_y == index], axis=0)\n",
    "    prob_std = np.std(kb_predictor.last_class_probabilities[train_y == index], axis=0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Standardised Value for {} Articles'.format(topic_code_to_topic_dict[topic_code]))\n",
    "    plt.xlabel('Standardised Value')\n",
    "    plt.xlim(-2.5, 2.5)\n",
    "    sns.barplot(x=((prob_mean-np.mean(prob_means, axis=0))/np.std(prob_means, axis=0)),\n",
    "                   y=list(int_to_topic.values()))\n",
    "    plt.savefig('standardised_{}.pdf'.format(topic_code_to_topic_dict[topic_code]), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse number of words from each topic for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n",
      "Wiki topic probabilies shape: (1, 300)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of topic to the total number of phrases matched in each article\n",
    "topic_num_phrases_matched = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Make a REST request to get Wikipedia topic probabilities from the classifier server\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    \n",
    "    # Prime the classifier with this example\n",
    "    kb_predictor.predict([train_x[i]])\n",
    "    \n",
    "    # Get the phrases that were matched from the document to the knowledge base\n",
    "    r = requests.get(url = 'http://127.0.0.1:5000/probabilities/{}'.format(-1))\n",
    "    phrase_to_prob = r.json()\n",
    "\n",
    "    # For each topic store the phrases that were matched\n",
    "    topics_to_phrases = defaultdict(set)\n",
    "\n",
    "    # Loop through phrases finding out their most prominent topic\n",
    "    for phrase in phrase_to_prob.keys():\n",
    "        # Phrases are prefixed with 'Phrase:'\n",
    "        doc = { 'text': phrase[7:] }\n",
    "        r = requests.post(url = 'http://127.0.0.1:5000/classify', json = doc) \n",
    "        wiki_topic_to_prob = r.json()\n",
    "        max_prob = 0\n",
    "        max_topic = 0\n",
    "        for topic, prob in wiki_topic_to_prob.items():\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                max_topic = topic\n",
    "        topics_to_phrases[max_topic].add(phrase[7:])\n",
    "\n",
    "    # Update topic to num phrases matched dictionary\n",
    "    for topic in wiki_topics_to_actual_topics.keys():\n",
    "        if topic in topics_to_phrases:\n",
    "            topic_num_phrases_matched[train_y[i]][topic].append(len(topics_to_phrases[topic]))\n",
    "        else:\n",
    "            topic_num_phrases_matched[train_y[i]][topic].append(0)\n",
    "\n",
    "\n",
    "# Plot graph of mean number of matches in each article for each topic class\n",
    "for index, topic_code in int_to_topic_code.items():\n",
    "    \n",
    "    # Calculate mean word count in each article for each topic\n",
    "    topics = []\n",
    "    topic_means = []\n",
    "    topic_stdevs = []\n",
    "    \n",
    "    for topic, counts in topic_num_phrases_matched[index].items():\n",
    "        topics.append(topic)\n",
    "        topic_means.append(np.mean(counts))\n",
    "        topic_stdevs.append(np.std(counts))\n",
    "\n",
    "    if topics and topic_means:\n",
    "        plt.figure()\n",
    "        plt.title('Mean Number of Word Matches for {} Articles'.format(topic_code_to_topic_dict[topic_code]))\n",
    "        plt.xlabel('Number of Word Matches')\n",
    "        plt.xlim(0.0, 30)\n",
    "        sns.barplot(x=topic_means, y=topics, xerr=topic_stdevs)\n",
    "        plt.savefig('topic_word_matches_{}.pdf'.format(topic_code_to_topic_dict[topic_code]), bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess unsupervised classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Making predictions for {} documents'.format(len(test_y)))\n",
    "predict_y = kb_predictor.predict(test_x)\n",
    "classification_report, confusion_matrix = kb_predictor.get_classification_report(test_y, predict_y)\n",
    "\n",
    "print(classification_report)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find examples where predictions went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_code, index in topic_code_to_int.items():\n",
    "    topic_subset = predict_y[test_y == index]\n",
    "    topic_subset_incorrect = topic_subset[topic_subset != index]\n",
    "    document_subset = test_x[test_y == index]\n",
    "    document_subset = document_subset[topic_subset != index]\n",
    "    \n",
    "    print('------ 5 random erroneous predictions for {} ------'.format(topic_code_to_topic_dict[topic_code]))\n",
    "    print('')\n",
    "    random_indices = np.random.choice(np.arange(len(topic_subset_incorrect)), 5)\n",
    "    for index in random_indices:\n",
    "        print(document_subset[index])\n",
    "        print('')\n",
    "        print('Above classified as {}'.format(topic_code_to_topic_dict[int_to_topic_code[topic_subset_incorrect[index]]]))\n",
    "        print('')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
